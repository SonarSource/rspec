Agents based on Large Language Models can behave in non-deterministic ways, and
making sure they behave safely and as intended is a significant challenge. Unlike
traditional software where inputs and outputs can be tightly controlled to ensure safe and predictable behavior.

This is especially true when the model is given "uncontrolled" instructions.
Users can intentionally or unintentionally, provide inputs that lead to
harmful, biased, or otherwise undesirable outputs. This can result in a range of
risks, from generating inappropriate content to taking unintended actions that
could compromise security or lead to financial loss.

The first risk is about the content the AI generates:

* Leakage of sensitive information the model was trained on, such as PII or proprietary business secrets.
* Harmful material such as hate speech, biased language, or explicit content.
* Misinformation due to "hallucinations" where the model confidently states false information.
* Unprofessional or off-brand responses that damage a company's reputation.

The second risk is specific to agents that can take actions, such as tools, API
calls or system interactions:

* Irreversible and destructive actions, such as permanent data deletion or unauthorized purchases.
* Excessive resource consumption leading to unexpected financial costs.
* Attack vectors for security vulnerabilities that could be exploited by malicious actors
