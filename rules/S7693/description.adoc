In a Large Language Model conversation, prompts are non-deterministic and it is difficult
to contain the model's behavior. This is especially true when the model is given
instructions to act autonomously, such as in an AI agent that can take actions (e.g., use tools, call APIs, interact with systems).

Users, can intentionally or unintentionally, provide inputs that lead to
harmful, biased, or otherwise undesirable outputs. This can result in a range of
risks, from generating inappropriate content to taking unintended actions that
could compromise security or lead to financial loss.

The first risk is about the content the AI generates:
Large language models (LLMs) present significant risks primarily through the
generation of unsafe, inaccurate, and inappropriate content. They can produce
harmful material such as hate speech and biased language, which perpetuates
unfair societal prejudices, causing direct harm to users and exposing
organizations to severe legal and reputational damage. Furthermore, a core
problem is their tendency for "hallucinations"—confidently stating false
information—which spreads misinformation, erodes user trust, and can lead to
disastrous real-world outcomes if used for critical tasks.

Beyond the content's nature, LLMs also pose major operational and security
risks. They may generate unprofessional or off-brand responses that damage
a company's reputation and create a poor user experience. Critically, these
models can also inadvertently leak sensitive information they were trained on,
such as personally identifiable information (PII) or proprietary business
secrets, leading to major privacy and security breaches.

The second risk is specific to AI agents that can take actions (e.g., use tools, call APIs, interact with systems):

They can misinterpret ambiguous user commands, leading to irreversible and
destructive actions such as permanent data deletion or unauthorized purchases.
This can result in severe data loss, financial costs, and business disruption.
Furthermore, an agent might get stuck in a repetitive loop of inefficient API
calls, causing excessive resource consumption and massive, unexpected financial
bills from service providers.

Beyond operational failures, AI agents are susceptible to security
vulnerabilities. Malicious actors can use techniques like "prompt injection" to
trick an agent into bypassing its instructions and executing unauthorized
commands. This could lead to the agent exfiltrating sensitive data, modifying
system files, or being used as a vector to launch further attacks on internal
infrastructure. Such exploits can result in a complete system compromise and
catastrophic data theft.

