This rule raises an issue if a data frame has duplicate column names.

== Why is this an issue?

In PySpark, a `DataFrame` with duplicate column names can cause ambiguous and unexpected results with join, transformation, and data retrieval operations. For example:

* Column selection becomes unpredictable: `df.select("name")` will raise an exception
* Joins with other DataFrames may produce unexpected results or errors 
* Saving to external data sources may fail

This rule ensures that a `DataFrame` is not constructed with duplicated column names.

== How to fix it
To fix this issue, remove or rename the duplicate columns.

=== Code examples

==== Noncompliant code example

[source,python,diff-id=1,diff-type=noncompliant]
----
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Example").getOrCreate()

data = [(1, "Alice", 28), (2, "Bob", 25)]
df = spark.createDataFrame(data, ["id", "name", "name"]) # Noncompliant: "name" is duplicated
----

==== Compliant solution

[source,python,diff-id=1,diff-type=compliant]
----
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Example").getOrCreate()

data = [(1, "Alice", 28), (2, "Bob", 25)]
df = spark.createDataFrame(data, ["id", "name", "age"]) # Compliant
----

== Resources
=== Documentation
- PySpark Documentation - https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html#do-not-use-duplicated-column-names[Best Practices]

ifdef::env-github,rspecator-view[]
=== Implementation Specification

At a minimum, this rule should raise when `createDataFrame(...)` is used with an array with duplicate string literals. 

There are a few cases, where the rule can be expanded. 

* `createDataFrame(...)` is quite complex and there are a lot of ways to create a DataFrame with it
** `createDataFrame(...)` with a dictionary (e.g. `createDataFrame([{"id": 2, "name": "Alice"}, {"id": 2, "name": "Bob"}])`)

** `createDataFrame(...)` can be given a string definition of the schema (e.g. `createDataFrame([('Alice', 1)], "name: string, name: int")`)

** `createDataFrame(...)` can be used with row objects (see below)
** `createDataFrame(...)` can be used with a schema (see below)

[source,python]
----
Person = Row("name", "name")
spark.createDataFrame([Person("Alice", 1), Person("Bob", 2)])
----

[source,python]
----
data = ...
schema = StructType([
   StructField("name", StringType(), True),
   StructField("name", StringType(), True),
   StructField("age", IntegerType(), True)])
spark.createDataFrame(data, schema).show()
----

Schemas can also be nested
[source,python]
----
nested_schema = StructType([
      StructField("id", IntegerType(), True),
      StructField("nested", StructType([
         StructField("field1", StringType(), True),
         StructField("field2", StringType(), True)
      ]), True)
])
----

In addition to that, parts can be passed as variable, instead of literals. This seems to be especially common for schemas.


=== Message

Rename or remove the duplicate columns in the data frame.

=== Highlighting

The main location is the `createDataFrame` and the secondary location is the duplicate column names.
endif::env-github,rspecator-view[]