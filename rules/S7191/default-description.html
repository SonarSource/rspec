<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule identifies instances where multiple <code>withColumn</code> calls are used in succession to add or modify columns in a PySpark DataFrame. It suggests using <code>withColumns</code> instead, which is more efficient and concise.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Using <code>withColumn</code> multiple times can lead to inefficient code, as each call creates a new Spark Logical Plan. withColumns allows for adding or modifying multiple columns in a single operation, improving performance.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Creating a new column can be a costly operation, as Spark has to loop on every row to compute the new column value.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exceptions">Exceptions</h3>
<div class="paragraph">
<p><code>withColumn</code> can be used multiple times sequentially on a Dataframe when computing consecutive columns requires the presence of the previous ones.
In this case, consecutive <code>withColumn</code> calls are a solution.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame([[1,2],[2,3]], ["id", "value"])
df_with_new_cols = df.withColumn("squared_value", col("value") * col("value")).withColumn("cubic_value", col("squared_value") * col("value")) # Compliant</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_to_fix_it">How to fix it</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To fix this issue, <code>withColumns</code> method should be used instead of multiple consecutive calls to <code>withColumn</code> method.</p>
</div>
<div class="sect2">
<h3 id="_code_examples">Code examples</h3>
<div class="sect3">
<h4 id="_noncompliant_code_example">Noncompliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame([[1,2],[2,3]], ["id", "value"])
df_with_new_cols = df.withColumn("value_plus_1", col("value") + 1).withColumn("value_plus_2", col("value") + 2).withColumn("value_plus_3", col("value") + 3) # Noncompliant</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_solution">Compliant solution</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame([[1,2],[2,3]], ["id", "value"])
df_with_new_cols = df.withColumns({    # Compliant
    "value_plus_1": col("value") + 1,
    "value_plus_2": col("value") + 2,
    "value_plus_3": col("value") + 3,
})</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_resources">Resources</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PySpark withColumn Documentation - <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html">pyspark.sql.DataFrame.withColumn</a></p>
</li>
<li>
<p>PySpark withColumns Documentation - <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html">pyspark.sql.DataFrame.withColumns</a></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_articles_blog_posts">Articles &amp; blog posts</h3>
<div class="ulist">
<ul>
<li>
<p>Medium blog - <a href="https://blog.devgenius.io/why-to-avoid-multiple-chaining-of-withcolumn-function-in-spark-job-35ee8e09daaa">Why to avoid multiple chaining of withColumn() function in Spark job.</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>