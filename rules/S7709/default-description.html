<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when <code>torch.tensor()</code> is used to convert a list or sequence of existing PyTorch tensors into a single tensor.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When you have a list of existing tensors and want to combine them into a single tensor, using <code>torch.tensor()</code> creates unnecessary memory overhead and can lead to out-of-memory errors.</p>
</div>
<div class="paragraph">
<p>The <code>torch.tensor()</code> constructor creates a completely new tensor by copying data from the input. When applied to a list of existing tensors, this means:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The original tensors remain in memory</p>
</li>
<li>
<p>A new tensor is created with duplicated data</p>
</li>
<li>
<p>Memory usage effectively doubles</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This memory duplication becomes particularly problematic when working with GPU tensors, where memory is often limited. Large tensor operations can quickly exhaust available CUDA memory, causing runtime errors.</p>
</div>
<div class="paragraph">
<p>The <code>torch.cat()</code> function is specifically designed for concatenating existing tensors. It efficiently combines tensors along a specified dimension without creating unnecessary copies of the original data.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Memory inefficiency can lead to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Increased memory consumption, potentially doubling memory usage</p>
</li>
<li>
<p>CUDA out-of-memory errors when working with GPU tensors</p>
</li>
<li>
<p>Slower performance due to unnecessary data copying</p>
</li>
<li>
<p>Application crashes in memory-constrained environments</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix_in_pytorch">How to fix in PyTorch?</h3>
<div class="paragraph">
<p>Replace <code>torch.tensor()</code> with <code>torch.cat()</code> when concatenating existing tensors. Specify the dimension along which to concatenate using the <code>dim</code> parameter.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

# Collect tensors in a list
tensor_list = []
for i in range(10):
    tensor_list.append(torch.randn(5, 10))

# Memory-inefficient: creates duplicate data
result = torch.tensor(tensor_list)  # Noncompliant</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

# Collect tensors in a list
tensor_list = []
for i in range(10):
    tensor_list.append(torch.randn(5, 10))

# Memory-efficient: concatenates without duplication
result = torch.cat(tensor_list, dim=0)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When working with GPU tensors, the memory efficiency of <code>torch.cat()</code> becomes even more critical to avoid CUDA out-of-memory errors.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_2">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

device = torch.device('cuda')
tensor_list = []
for i in range(100):
    tensor_list.append(torch.randn(1000, 1000).to(device))

# Risk of CUDA OOM due to memory duplication
result = torch.tensor(tensor_list)  # Noncompliant</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_2">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

device = torch.device('cuda')
tensor_list = []
for i in range(100):
    tensor_list.append(torch.randn(1000, 1000).to(device))

# Efficient concatenation without memory duplication
result = torch.cat(tensor_list, dim=0)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch torch.cat documentation - <a href="https://pytorch.org/docs/stable/generated/torch.cat.html">Official documentation for torch.cat function</a></p>
</li>
<li>
<p>PyTorch torch.tensor documentation - <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html">Official documentation for torch.tensor constructor</a></p>
</li>
<li>
<p>PyTorch Memory Management - <a href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management">Guide to PyTorch CUDA memory management best practices</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>