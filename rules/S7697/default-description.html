<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when a PyTorch tensor operation that returns a new tensor is called without assigning the result to a variable.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>PyTorch tensor operations like <code>abs()</code>, <code>clamp()</code>, <code>relu()</code>, and many others return new tensors rather than modifying the original tensor. When you call these methods without assigning the result, the operation is performed but the result is immediately discarded.</p>
</div>
<div class="paragraph">
<p>This creates a silent bug where the code appears to perform an operation but has no actual effect on the program state. The original tensor remains unchanged, which can lead to incorrect computations in machine learning models.</p>
</div>
<div class="paragraph">
<p>For example, if you call <code>tensor.abs()</code> without assignment, the tensor still contains the original values including any negative numbers. This can cause unexpected behavior in neural networks where you intended to apply the absolute value function.</p>
</div>
<div class="paragraph">
<p>The confusion often arises because some operations in other libraries (like NumPy) or some PyTorch operations do modify tensors in-place by default, leading developers to assume all operations work the same way.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>This issue can cause silent logical errors in machine learning models. The tensor operations appear to execute successfully but don&#8217;t actually modify the data as intended. This can lead to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Incorrect model training with unexpected tensor values</p>
</li>
<li>
<p>Poor model performance due to unintended data preprocessing</p>
</li>
<li>
<p>Difficult-to-debug issues since the code runs without errors</p>
</li>
<li>
<p>Wasted computational resources performing operations whose results are discarded</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix_in_pytorch">How to fix in PyTorch?</h3>
<div class="paragraph">
<p>Assign the result of the tensor operation to a variable. This ensures the transformed tensor is captured and can be used in subsequent operations.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
tensor = torch.tensor([-1.0, 2.0, -3.0])
tensor.abs()  # Noncompliant: result not assigned
print(tensor)  # Still contains negative values</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
tensor = torch.tensor([-1.0, 2.0, -3.0])
tensor = tensor.abs()  # Assign result back to variable
print(tensor)  # Now contains absolute values</code></pre>
</div>
</div>
<div class="paragraph">
<p>Use the in-place variant of the operation by adding an underscore suffix. This modifies the original tensor directly and doesn&#8217;t return a new tensor.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_2">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
data = torch.tensor([2.5, 3.7, 1.2, 0.8])
data.clamp(0, 1)  # Noncompliant: result not assigned</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_2">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
data = torch.tensor([2.5, 3.7, 1.2, 0.8])
data.clamp_(0, 1)  # In-place operation modifies original tensor</code></pre>
</div>
</div>
<div class="paragraph">
<p>For functional operations, assign the result to a new variable or back to the original variable.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_3">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
values = torch.tensor([-2.0, 1.5, -0.5])
torch.relu(values)  # Noncompliant: result not assigned</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_3">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
values = torch.tensor([-2.0, 1.5, -0.5])
values = torch.relu(values)  # Assign result back to variable</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch Tensor Operations - <a href="https://pytorch.org/docs/stable/tensors.html">Official PyTorch documentation on tensor operations and their behavior</a></p>
</li>
<li>
<p>PyTorch In-place Operations - <a href="https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd">Documentation on in-place operations and their implications for automatic differentiation</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>