<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when PyTorch InstanceNorm layers (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d) are used without explicitly setting the <code>affine</code> parameter.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>PyTorch&#8217;s InstanceNorm layers have an inconsistent default behavior compared to other normalization layers. InstanceNorm layers default to <code>affine=False</code>, meaning they do not include learnable scale and shift parameters. This differs from BatchNorm and GroupNorm layers, which default to <code>affine=True</code>.</p>
</div>
<div class="paragraph">
<p>This inconsistency can lead to several problems:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Unexpected model behavior</strong>: Developers familiar with BatchNorm might assume InstanceNorm behaves similarly, leading to models that don&#8217;t learn as expected.</p>
</li>
<li>
<p><strong>Debugging difficulties</strong>: When models underperform, the missing affine parameters might not be immediately obvious as the cause.</p>
</li>
<li>
<p><strong>Inconsistent layer behavior</strong>: Having some normalization layers with learnable parameters and others without can create unintended architectural differences.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>affine</code> parameter controls whether the layer includes learnable scale (gamma) and shift (beta) parameters that are applied after normalization. When <code>affine=False</code>, the layer only performs the normalization step without these additional learnable transformations.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Models may not train as expected due to missing learnable parameters in InstanceNorm layers. This can result in reduced model performance, slower convergence, or unexpected training dynamics. The issue is particularly problematic because it creates silent behavioral differences that may not be immediately apparent during development.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix">How to fix?</h3>
<div class="paragraph">
<p>Explicitly set the <code>affine</code> parameter when creating InstanceNorm layers. Set it to <code>True</code> if you want learnable scale and shift parameters (similar to BatchNorm behavior), or <code>False</code> if you want only normalization without learnable parameters.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch.nn as nn

# Relies on default affine=False
norm = nn.InstanceNorm2d(64)  # Noncompliant</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch.nn as nn

# Explicitly set affine parameter
norm = nn.InstanceNorm2d(64, affine=True)  # or affine=False if intended</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch InstanceNorm Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html">Official PyTorch documentation for InstanceNorm layers</a></p>
</li>
<li>
<p>PyTorch Normalization Layers Guide - <a href="https://pytorch.org/docs/stable/nn.html#normalization-layers">Overview of all normalization layers in PyTorch</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>