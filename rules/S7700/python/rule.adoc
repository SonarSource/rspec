This rule raises an issue when PyTorch InstanceNorm layers (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d) are used without explicitly setting the `affine` parameter.

== Why is this an issue?

PyTorch's InstanceNorm layers have an inconsistent default behavior compared to other normalization layers. InstanceNorm layers default to `affine=False`, meaning they do not include learnable scale and shift parameters. This differs from BatchNorm and GroupNorm layers, which default to `affine=True`.

This inconsistency can lead to several problems:

* **Unexpected model behavior**: Developers familiar with BatchNorm might assume InstanceNorm behaves similarly, leading to models that don't learn as expected.
* **Debugging difficulties**: When models underperform, the missing affine parameters might not be immediately obvious as the cause.
* **Inconsistent layer behavior**: Having some normalization layers with learnable parameters and others without can create unintended architectural differences.

The `affine` parameter controls whether the layer includes learnable scale (gamma) and shift (beta) parameters that are applied after normalization. When `affine=False`, the layer only performs the normalization step without these additional learnable transformations.

=== What is the potential impact?

Models may not train as expected due to missing learnable parameters in InstanceNorm layers. This can result in reduced model performance, slower convergence, or unexpected training dynamics. The issue is particularly problematic because it creates silent behavioral differences that may not be immediately apparent during development.

=== How to fix?


Explicitly set the `affine` parameter when creating InstanceNorm layers. Set it to `True` if you want learnable scale and shift parameters (similar to BatchNorm behavior), or `False` if you want only normalization without learnable parameters.

==== Non-compliant code example

[source,python,diff-id=1,diff-type=noncompliant]
----
import torch.nn as nn

# Relies on default affine=False
norm = nn.InstanceNorm2d(64)  # Noncompliant
----

==== Compliant code example

[source,python,diff-id=1,diff-type=compliant]
----
import torch.nn as nn

# Explicitly set affine parameter
norm = nn.InstanceNorm2d(64, affine=True)  # or affine=False if intended
----

=== Documentation

 * PyTorch InstanceNorm Documentation - https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html[Official PyTorch documentation for InstanceNorm layers]
 * PyTorch Normalization Layers Guide - https://pytorch.org/docs/stable/nn.html#normalization-layers[Overview of all normalization layers in PyTorch]

