<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when copying existing PyTorch tensors using deprecated methods like <code>torch.tensor()</code> or <code>tensor.new_tensor()</code> instead of the recommended <code>tensor.clone().detach()</code> or <code>tensor.detach().clone()</code>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>PyTorch provides several ways to copy tensors, but not all methods are equally efficient or clear in their intent.</p>
</div>
<div class="paragraph">
<p>When you use <code>torch.tensor(existing_tensor)</code> or <code>tensor.new_tensor(existing_tensor)</code> to copy an existing tensor, PyTorch issues a UserWarning because these methods are not optimized for copying existing tensors. They were designed for creating new tensors from raw data, not for duplicating existing tensor objects.</p>
</div>
<div class="paragraph">
<p>The recommended methods <code>tensor.clone().detach()</code> and <code>tensor.detach().clone()</code> are specifically designed for tensor copying. They clearly express your intent to create a copy of an existing tensor and remove it from the computation graph. This makes your code more readable and helps other developers understand what you&#8217;re trying to accomplish.</p>
</div>
<div class="paragraph">
<p>There&#8217;s also a subtle performance difference between the two recommended approaches. Using <code>tensor.detach().clone()</code> is slightly more efficient than <code>tensor.clone().detach()</code> because it detaches the tensor from the computation graph first, avoiding the need to copy gradient information that will be discarded anyway.</p>
</div>
<div class="paragraph">
<p>Using only <code>tensor.clone()</code> without <code>detach()</code> keeps the tensor connected to the computation graph, which may cause unexpected gradient propagation and memory usage if that&#8217;s not your intention.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Using deprecated tensor copying methods can lead to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Performance degradation due to suboptimal copying mechanisms</p>
</li>
<li>
<p>UserWarnings cluttering your application logs</p>
</li>
<li>
<p>Less readable code that doesn&#8217;t clearly express the intent to copy and detach tensors</p>
</li>
<li>
<p>Potential memory leaks if tensors remain connected to computation graphs unintentionally</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix_in_pytorch">How to fix in PyTorch?</h3>
<div class="paragraph">
<p>Replace <code>torch.tensor()</code> calls with <code>tensor.detach().clone()</code> when copying existing tensors. This approach is more efficient and avoids UserWarnings.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = torch.tensor(x)  # Noncompliant: triggers UserWarning</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = x.detach().clone()  # Creates a copy without computation graph</code></pre>
</div>
</div>
<div class="paragraph">
<p>Replace <code>tensor.new_tensor()</code> calls with <code>tensor.detach().clone()</code> for better performance and clearer intent.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_2">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = x.new_tensor(x)  # Noncompliant: triggers UserWarning</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_2">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = x.detach().clone()  # More efficient and explicit</code></pre>
</div>
</div>
<div class="paragraph">
<p>When you need gradients on the copied tensor, use <code>detach().clone().requires_grad_(True)</code> to explicitly enable gradient computation.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_3">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = torch.tensor(x, requires_grad=True)  # Noncompliant: deprecated approach</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_3">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(3, 4, requires_grad=True)
y = x.detach().clone().requires_grad_(True)  # Explicit and recommended</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch tensor.clone() documentation - <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html">Official documentation for the clone() method</a></p>
</li>
<li>
<p>PyTorch tensor.detach() documentation - <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html">Official documentation for the detach() method</a></p>
</li>
<li>
<p>PyTorch torch.tensor() documentation - <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html">Official documentation explaining when to use torch.tensor() vs clone().detach()</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>