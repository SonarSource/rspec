This rule raises an issue when tensors are manually expanded using the `expand()` method before element-wise operations, where PyTorch's automatic broadcasting would handle the dimension matching.

== Why is this an issue?

PyTorch provides automatic broadcasting for element-wise operations between tensors with compatible shapes. Broadcasting allows operations between tensors of different dimensions without explicitly expanding them.

When you manually expand tensors using the `expand()` method before element-wise operations, you create unnecessary memory overhead. The `expand()` method creates a view of the tensor with repeated elements, which consumes additional memory even though the underlying data doesn't change.

Automatic broadcasting is more efficient because:

* It avoids creating intermediate expanded tensor views
* It reduces memory consumption by working directly with the original tensor shapes
* It performs the same mathematical operation with better performance
* It results in cleaner, more readable code

Broadcasting works when tensor dimensions are compatible according to PyTorch's broadcasting rules: dimensions are compared element-wise from the trailing dimension, and they are compatible if they are equal, one of them is 1, or one of them doesn't exist.

=== What is the potential impact?

Manual tensor expansion before element-wise operations leads to unnecessary memory consumption and reduced performance. In machine learning workflows with large tensors, this can cause memory pressure and slower training times. The inefficiency becomes more significant with larger tensor sizes and can impact the scalability of machine learning models.

=== How to fix?


Remove the manual `expand()` calls and let PyTorch's automatic broadcasting handle the dimension matching. The element-wise operation will work directly on the original tensors.

==== Noncompliant code example

[source,python,diff-id=1,diff-type=noncompliant]
----
import torch
x = torch.randn(3, 1, 4)
y = torch.randn(1, 5, 1)
expanded_x = x.expand(3, 5, 4)  # Noncompliant
expanded_y = y.expand(3, 5, 4)  # Noncompliant
result = expanded_x * expanded_y
----

==== Compliant code example

[source,python,diff-id=1,diff-type=compliant]
----
import torch
x = torch.randn(3, 1, 4)
y = torch.randn(1, 5, 1)
result = x * y  # Broadcasting handles dimension matching automatically
----

=== Documentation

 * PyTorch Broadcasting Semantics - https://pytorch.org/docs/stable/notes/broadcasting.html[Official PyTorch documentation explaining broadcasting rules and behavior]
 * PyTorch Tensor.expand() Documentation - https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html[Documentation for the expand() method and its memory implications]

