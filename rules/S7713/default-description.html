<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when tensors are manually expanded using the <code>expand()</code> method before element-wise operations, where PyTorch&#8217;s automatic broadcasting would handle the dimension matching.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>PyTorch provides automatic broadcasting for element-wise operations between tensors with compatible shapes. Broadcasting allows operations between tensors of different dimensions without explicitly expanding them.</p>
</div>
<div class="paragraph">
<p>When you manually expand tensors using the <code>expand()</code> method before element-wise operations, you create unnecessary memory overhead. The <code>expand()</code> method creates a view of the tensor with repeated elements, which consumes additional memory even though the underlying data doesn&#8217;t change.</p>
</div>
<div class="paragraph">
<p>Automatic broadcasting is more efficient because:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It avoids creating intermediate expanded tensor views</p>
</li>
<li>
<p>It reduces memory consumption by working directly with the original tensor shapes</p>
</li>
<li>
<p>It performs the same mathematical operation with better performance</p>
</li>
<li>
<p>It results in cleaner, more readable code</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Broadcasting works when tensor dimensions are compatible according to PyTorch&#8217;s broadcasting rules: dimensions are compared element-wise from the trailing dimension, and they are compatible if they are equal, one of them is 1, or one of them doesn&#8217;t exist.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Manual tensor expansion before element-wise operations leads to unnecessary memory consumption and reduced performance. In machine learning workflows with large tensors, this can cause memory pressure and slower training times. The inefficiency becomes more significant with larger tensor sizes and can impact the scalability of machine learning models.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix">How to fix?</h3>
<div class="paragraph">
<p>Remove the manual <code>expand()</code> calls and let PyTorch&#8217;s automatic broadcasting handle the dimension matching. The element-wise operation will work directly on the original tensors.</p>
</div>
<div class="sect3">
<h4 id="_noncompliant_code_example">Noncompliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
x = torch.randn(3, 1, 4)
y = torch.randn(1, 5, 1)
expanded_x = x.expand(3, 5, 4)  # Noncompliant
expanded_y = y.expand(3, 5, 4)  # Noncompliant
result = expanded_x * expanded_y</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
x = torch.randn(3, 1, 4)
y = torch.randn(1, 5, 1)
result = x * y  # Broadcasting handles dimension matching automatically</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch Broadcasting Semantics - <a href="https://pytorch.org/docs/stable/notes/broadcasting.html">Official PyTorch documentation explaining broadcasting rules and behavior</a></p>
</li>
<li>
<p>PyTorch Tensor.expand() Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html">Documentation for the expand() method and its memory implications</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>