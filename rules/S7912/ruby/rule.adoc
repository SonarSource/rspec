This is an issue when background jobs or asynchronous operations lack proper retry mechanisms, exponential backoff, or structured error handling, making them vulnerable to transient failures.

== Why is this an issue?

Background jobs and asynchronous operations are particularly susceptible to transient failures such as network timeouts, temporary service unavailability, or rate limiting. Without proper retry mechanisms, these failures can result in lost data, incomplete operations, or require manual intervention to resolve.

When jobs fail without retry logic, they typically end up in a failed state and need manual reprocessing. This creates operational overhead and can lead to data inconsistencies if some operations succeed while others fail.

Simple retry mechanisms without exponential backoff can overwhelm already struggling services by immediately retrying failed operations. This can worsen the situation by contributing to cascading failures or triggering rate limiting mechanisms.

Exponential backoff is crucial because it gradually increases the delay between retry attempts, giving temporary issues time to resolve while reducing load on external services. This approach is more respectful of downstream services and increases the likelihood of eventual success.

Proper error logging is essential for monitoring and debugging. Without structured error handling, it becomes difficult to identify patterns in failures, track job success rates, or diagnose underlying issues that may require code changes or infrastructure improvements.

=== What is the potential impact?

Applications may experience data loss, incomplete operations, or require frequent manual intervention when background jobs fail. Without exponential backoff, failed retries can overwhelm external services, potentially causing cascading failures or triggering rate limiting that affects other parts of the system. Poor error handling makes it difficult to monitor job health and diagnose issues, leading to longer resolution times and reduced system reliability.

== How to fix it

For custom retry logic, implement exponential backoff with proper error handling and logging. Track retry attempts and implement a maximum retry limit to prevent infinite loops.

=== Code examples

==== Noncompliant code example

[source,ruby,diff-id=1,diff-type=noncompliant]
----
def sync_data
  3.times do
    begin
      perform_sync
      return
    rescue => error
      Rails.logger.error "Sync failed: #{error}"
      sleep(1)
    end
  end
end
----

==== Compliant solution

[source,ruby,diff-id=1,diff-type=compliant]
----
def sync_data
  max_retries = 3
  retries = 0
  
  while retries < max_retries
    begin
      perform_sync
      return
    rescue => error
      Rails.logger.error "Sync failed (attempt #{retries + 1}/#{max_retries}): #{error.message}"
      retries += 1
      sleep(2 ** retries) if retries < max_retries # exponential backoff
    end
  end
  
  Rails.logger.error "Sync failed after #{max_retries} attempts"
end
----

== How to fix it in Rails

Use Rails' built-in retry_on mechanism with exponential backoff for ApplicationJob classes. Add proper error logging while still allowing the retry mechanism to function.

=== Code examples

==== Noncompliant code example

[source,ruby,diff-id=2,diff-type=noncompliant]
----
class DigestWorker < ApplicationJob
  def perform(user_id)
    send_digest(user_id)
  end
end
----

==== Compliant solution

[source,ruby,diff-id=2,diff-type=compliant]
----
class DigestWorker < ApplicationJob
  retry_on StandardError, wait: :exponentially_longer, attempts: 5
  
  def perform(user_id)
    send_digest(user_id)
  rescue => error
    Rails.logger.error "Digest failed for user #{user_id}: #{error.message}"
    raise
  end
end
----

== Resources

=== Documentation

 * Rails Active Job Retry Mechanisms - https://guides.rubyonrails.org/active_job_basics.html#retrying-or-discarding-failed-jobs[Official Rails documentation on implementing retry logic in Active Job]

 * Exponential Backoff Pattern - https://cloud.google.com/iot/docs/how-tos/exponential-backoff[Google Cloud documentation explaining exponential backoff strategies]

 * Sidekiq Retry Logic - https://github.com/mperham/sidekiq/wiki/Error-Handling[Sidekiq documentation on error handling and retry mechanisms]

=== Standards

 * CWE-754: Improper Check for Unusual or Exceptional Conditions - https://cwe.mitre.org/data/definitions/754.html[Addresses the importance of proper error handling in software systems]
