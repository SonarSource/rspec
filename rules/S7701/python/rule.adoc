This rule raises an issue when a dropout layer is defined in a PyTorch `nn.Module` class but is never called in the `forward` method.

== Why is this an issue?

Dropout is a regularization technique that randomly sets some neurons to zero during training to prevent overfitting. When you define a dropout layer in your neural network's `__init__` method but don't use it in the `forward` method, the dropout has no effect.

This creates several problems:

* **Dead code**: The dropout layer exists but serves no purpose, making the code confusing and misleading.
* **Missing regularization**: Your model won't benefit from the intended regularization, potentially leading to overfitting.
* **Maintenance confusion**: Other developers (or future you) might assume dropout is being applied when it's not.
* **Performance overhead**: The unused layer still consumes memory and may affect model serialization.

Dropout layers must be explicitly called in the forward pass to take effect. Simply defining them is not enough.

=== What is the potential impact?

Without proper dropout usage, your neural network may overfit to the training data, leading to poor generalization on new, unseen data. This can result in models that perform well during training but fail in production environments.

=== How to fix in PyTorch?

Call the dropout layer in the forward method where you want regularization to be applied, typically after activation functions and before the next layer.

==== Non-compliant code example

[source,python,diff-id=1,diff-type=noncompliant]
----
import torch.nn as nn
import torch.nn.functional as F

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, p=0.5):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(p)  # Noncompliant

    def forward(self, x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out
----

==== Compliant code example

[source,python,diff-id=1,diff-type=compliant]
----
import torch.nn as nn
import torch.nn.functional as F

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, p=0.5):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(p)

    def forward(self, x):
        out = F.relu(self.fc1(x))
        out = self.dropout(out)  # Apply dropout after activation
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out
----

If you don't need the dropout layer, remove it entirely to clean up your code.

==== Non-compliant code example

[source,python,diff-id=2,diff-type=noncompliant]
----
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, output_size)
        self.dropout = nn.Dropout(0.3)  # Noncompliant

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
----

==== Compliant code example

[source,python,diff-id=2,diff-type=compliant]
----
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, output_size)
        # Removed unused dropout layer

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
----

=== Documentation

 * PyTorch Dropout Documentation - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html[Official PyTorch documentation for the Dropout layer]
 * PyTorch Neural Network Tutorial - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html[Tutorial on building neural networks in PyTorch, including proper layer usage]

=== Standards

 * CWE-561: Dead Code - https://cwe.mitre.org/data/definitions/561.html[Weakness related to code that is never executed or used]

