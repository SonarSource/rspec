<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when softmax activation is applied to model outputs before passing them to CrossEntropyLoss, or when softmax layers are included in model architectures that use CrossEntropyLoss for training.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>CrossEntropyLoss in PyTorch internally combines LogSoftmax and NLLLoss operations for optimal numerical stability. When you apply softmax activation before CrossEntropyLoss, you create a redundant computation chain that can degrade model performance.</p>
</div>
<div class="paragraph">
<p>The mathematical issue occurs because CrossEntropyLoss expects raw logits (unbounded values from the final linear layer) as input. It then applies LogSoftmax internally, which is numerically stable even for large input values. When you manually apply softmax first, the loss function receives probability values between 0 and 1, then applies LogSoftmax again, creating a "double softmax" effect.</p>
</div>
<div class="paragraph">
<p>This double application reduces gradient magnitudes during backpropagation. Smaller gradients mean slower learning and can prevent the model from reaching optimal performance. Additionally, the intermediate softmax step can introduce numerical instability, especially when dealing with very small probability values that approach zero.</p>
</div>
<div class="paragraph">
<p>The correct approach is to let your model output raw logits and pass them directly to CrossEntropyLoss. The loss function handles the softmax transformation internally using the more stable LogSoftmax operation.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>This anti-pattern can significantly impact model training performance. Reduced gradient magnitudes slow down learning, potentially requiring more training epochs to reach convergence. In some cases, the model may fail to learn effectively at all, resulting in poor accuracy on validation and test datasets. The numerical instability can also cause training loss to fluctuate unpredictably or even diverge in extreme cases.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix_in_pytorch">How to fix in PyTorch?</h3>
<div class="paragraph">
<p>Remove manual softmax calls before CrossEntropyLoss. Pass raw logits directly to the loss function.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
import torch.nn as nn

logits = model(input)
probs = torch.softmax(logits, dim=1)  # Noncompliant
loss = nn.CrossEntropyLoss()(probs, target)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
import torch.nn as nn

logits = model(input)
loss = nn.CrossEntropyLoss()(logits, target)  # Pass raw logits directly</code></pre>
</div>
</div>
<div class="paragraph">
<p>Remove softmax layers from model architectures when using CrossEntropyLoss. The final layer should output raw logits.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_2">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10),
    nn.Softmax(dim=1)  # Noncompliant
)
loss = nn.CrossEntropyLoss()(model(inputs), targets)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_2">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)  # No softmax layer
)
loss = nn.CrossEntropyLoss()(model(inputs), targets)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Use CrossEntropyLoss directly instead of manually combining softmax with other loss functions.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_3">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
import torch.nn.functional as F

logits = model(x)
probs = F.softmax(logits, dim=1)  # Noncompliant
loss = F.nll_loss(torch.log(probs), targets)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_3">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
import torch.nn.functional as F

logits = model(x)
loss = F.cross_entropy(logits, targets)  # Use cross_entropy directly</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch CrossEntropyLoss Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">Official documentation explaining that CrossEntropyLoss combines LogSoftmax and NLLLoss</a></p>
</li>
<li>
<p>PyTorch Softmax Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">Documentation for the Softmax activation function and its proper usage</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>