<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when PyTorch <code>optimizer.step()</code> and <code>loss.backward()</code> is used without <code>optimizer.zero_grad()</code>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In PyTorch the training loop of a neural network is comprised of several steps, not necessarily in this order:
* Forward pass, to pass the data through the model and output predictions
* Loss computation, to compute the loss based on the predictions and the ground truth
* Backward pass, to compute the gradient loss with the <code>loss.backward()</code> method
* Weights update, to update the model weights with the <code>optimizer.step()</code> method
* Gradients zeroed out, to prevent the gradients to accumulate with the <code>optimizer.zero_grad()</code> method</p>
</div>
<div class="paragraph">
<p>When training a model it is important to reset gradients for each training loop.
Failing to do so will accumulate the gradients which could skew the results and lead to poor performance.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_to_fix_it">How to fix it</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To fix the issue, call the <code>optimizer.zero_grad()</code> method.</p>
</div>
<div class="sect2">
<h3 id="_code_examples">Code examples</h3>
<div class="sect3">
<h4 id="_noncompliant_code_example">Noncompliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
from my_dataset import trainset
from my_model import NeuralNetwork
from torch.utils.data import DataLoader

trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

model = NeuralNetwork()

loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(100):
  for data, labels in trainloader:
      output = model(data)
      loss = loss_fn(output, labels)
      loss.backward()
      optimizer.step() # Noncompliant: optimizer.zero_grad() was not called in the training loop</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_solution">Compliant solution</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
from my_dataset import trainset
from my_model import NeuralNetwork
from torch.utils.data import DataLoader

trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

model = NeuralNetwork()

loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(100):
  for data, labels in trainloader:
      optimizer.zero_grad()
      output = model(data)
      loss = loss_fn(output, labels)
      loss.backward()
      optimizer.step()</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_resources">Resources</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch Documentation - <a href="https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop">The Training Loop</a></p>
</li>
<li>
<p>PyTorch Documentation - <a href="https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html#zeroing-out-gradients-in-pytorch">Zeroing out gradients in PyTorch</a></p>
</li>
<li>
<p>PyTorch Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch-optim-optimizer-zero-grad">torch.optim.Optimizer.zero_grad - reference</a></p>
</li>
<li>
<p>PyTorch Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch-optim-optimizer-step">torch.optim.Optimizer.step - reference</a></p>
</li>
<li>
<p>PyTorch Documentation - <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch-tensor-backward">torch.Tensor.backward - reference</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>(visible only on this page)</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_implementation_specification">Implementation specification</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Only in a loop if an optimizer.step() is called and loss.backward() is called, we shall raise the issue.</p>
</div>
<div class="sect2">
<h3 id="_message">Message</h3>
<div class="paragraph">
<p>Primary: Call the {optimizer name}.zero_grad() method</p>
</div>
</div>
<div class="sect2">
<h3 id="_issue_location">Issue location</h3>
<div class="paragraph">
<p>Primary : The {optimizer name}.step() method</p>
</div>
</div>
<div class="sect2">
<h3 id="_quickfix">Quickfix</h3>
<div class="paragraph">
<p>No</p>
</div>
</div>
</div>
</div>