Constructing prompt from user input is security-sensitive.

Control over a part of the prompt might be enough for an attacker to modify the behavior of the LLM and make it act with no respect to the constraints it is supposed to follow.

