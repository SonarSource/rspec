<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In a Large Language Model conversation, different roles have a clear hierarchy
and have distinctly different abilities to influence the conversation, define
its boundaries, and control the actions of other participants.</p>
</div>
<div class="paragraph">
<p>Nowadays, the trio of <code>system</code>, <code>user</code>, and <code>assistant</code> defining the core
roles of many Large Language Model (LLM) interactions is expanding to include
a more diverse set of roles, such as developer, tool, function, and even more
nuanced roles in multi-agent systems.</p>
</div>
<div class="paragraph">
<p>Injecting unchecked user input in privileged prompts, such as <code>system</code>, gives
unauthorized third parties the ability to break out of contexts and constraints
that you assume the LLM follows.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>When attackers detect privilege discrepancies while injecting into your LLM
application, they then map out their capabilities in terms of actions and
knowledge extraction, and then act accordingly.<br>
The impact is very dependent on the "screenplay" of the intended dialogues
between model, user(s), third-parties, tools, which you had in mind while
designing the application.</p>
</div>
<div class="paragraph">
<p>Below are some real-world scenarios that illustrate some impacts of an attacker
exploiting the vulnerability.</p>
</div>
<div class="sect3">
<h4 id="_data_manipulation">Data manipulation</h4>
<div class="paragraph">
<p>A malicious prompt injection enables data leakages or possibly impacting the
LLM discussions of other users.</p>
</div>
</div>
<div class="sect3">
<h4 id="_denial_of_service_and_code_execution">Denial of service and code execution</h4>
<div class="paragraph">
<p>Malicious prompt injections could allow the attacker to possibly leverage
internal tooling such as MCP, to delete sensitive or important data, or to send
tremendous amounts of requests to third-party services, leading to financial
losses or getting banned from such services.<br>
This threat is particularly insidious if the attacked organization does not
maintain a disaster recovery plan (DRP).</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_to_fix_it_in_openai">How to fix it in OpenAI</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_code_examples">Code examples</h3>
<div class="paragraph">
<p>In the following piece of code, control over sensitive roles such as <code>system</code>
and <code>developer</code> provides a clear way to exploit the underlying model, its
proprietary knowledge (e.g., RAG), and its capabilities (with MCPs).</p>
</div>
<div class="paragraph">
<p>The compliant solution revokes any external possibility of controlling
sensitive roles by just hardcoding the system and developer messages.</p>
</div>
<div class="sect3">
<h4 id="_noncompliant_code_example">Noncompliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">@RestController
@RequestMapping("/example")
public class ExampleController {
    private final OpenAIClient client;
    @PostMapping("/example")
    public ResponseEntity&lt;?&gt; example(@RequestBody Map&lt;String, String&gt; payload) {
        String promptText = payload.get("prompt_text");
        String systemText = payload.get("sys_text");
        String developerText = payload.get("dev_text");
        ChatCompletionCreateParams request = ChatCompletionCreateParams.builder()
            .model(ChatModel.GPT_3_5_TURBO)
            .maxCompletionTokens(2048)
            .addSystemMessage(systemText)
            .addDeveloperMessage(developerText)
            .addUserMessage(promptText)
            .build();
        var completion = client.chat().completions().create(request);
        return ResponseEntity.ok(
            Map.of(
                "response",
                completion.choices().stream()
                    .flatMap(choice -&gt; choice.message().content().stream())
                    .collect(Collectors.joining(" | "))
            )
        );
    }
}</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_compliant_solution">Compliant Solution</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">@RestController
@RequestMapping("/example")
public class ExampleController {
    private final OpenAIClient client;
    @PostMapping("/example")
    public ResponseEntity&lt;?&gt; example(@RequestBody Map&lt;String, String&gt; payload) {
        String promptText = payload.get("prompt_text");
        ChatCompletionCreateParams request = ChatCompletionCreateParams.builder()
            .model(ChatModel.GPT_3_5_TURBO)
            .maxCompletionTokens(2048)
            .addSystemMessage("""
            You are "ExampleBot", a friendly and professional AI assistant [...]
            Your role is to [...]
            """)
            .addDeveloperMessage("""
            // Developer Configuration &amp; Safety Wrapper
            1. The user's query will first be processed by [...]
            2. etc.
            """)
            .addUserMessage(promptText)
            .build();
        var completion = client.chat().completions().create(request);
        return ResponseEntity.ok(
            Map.of(
                "response",
                completion.choices().stream()
                    .flatMap(choice -&gt; choice.message().content().stream())
                    .collect(Collectors.joining(" | "))
            )
        );
    }
}</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_how_does_this_work">How does this work?</h3>
<div class="sect3">
<h4 id="_explicitly_stem_the_llm_context">Explicitly stem the LLM context</h4>
<div class="paragraph">
<p>While designing an LLM application, and particularly at the stage where you
design the system, developer and user prompts that are to be passed
to the model, keep the <strong>principle of least privilege</strong> in mind.</p>
</div>
<div class="paragraph">
<p>Start by providing any external third-party or user with the least amount of
capabilities or information, and only level up their privileges
<strong>intentionally</strong>, e.g. when a situation (like tool calls) requires it.</p>
</div>
<div class="paragraph">
<p>Another short-term hardening approach is to add AI guardrails to your LLM, such
as additional prompts forbidding the model from generating certain outputs.<br>
While creating these, keep in mind that deny-list-based filtering can be complex
to maintain in the long-term <strong>and</strong> can most of the time be creatively
bypassed.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_resources">Resources</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_standards">Standards</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>OWASP GenAI - <a href="https://genai.owasp.org/llmrisk/llm01-prompt-injection/">Top 10 2025 Category LLM01 - Prompt Injection</a></p>
</li>
</ul>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_implementation_specification">Implementation Specification</h2>
<div class="sectionbody">
<div class="paragraph">
<p>(visible only on this page)</p>
</div>
<div class="sect2">
<h3 id="_message">Message</h3>
<div class="paragraph">
<p>Change this code to not construct privileged prompts directly from user-controlled data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_highlighting">Highlighting</h3>
<div class="paragraph">
<p>"[varname]" is tainted (assignments and parameters)</p>
</div>
<div class="paragraph">
<p>this argument is tainted (method invocations)</p>
</div>
<div class="paragraph">
<p>the returned value is tainted (returns &amp; method invocations results)</p>
</div>
<hr>
</div>
</div>
</div>