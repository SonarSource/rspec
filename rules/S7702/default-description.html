<div class="sect1">
<h2 id="_description">Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This rule raises an issue when <code>torch.flatten()</code> or <code>tensor.flatten()</code> is called without specifying the <code>start_dim</code> parameter.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_is_this_an_issue">Why is this an issue?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>PyTorch&#8217;s <code>torch.flatten()</code> and <code>tensor.flatten()</code> methods flatten all dimensions by default, starting from dimension 0. This includes the batch dimension, which is usually not intended.</p>
</div>
<div class="paragraph">
<p>When working with batched data in neural networks, the first dimension typically represents the batch size. Flattening this dimension results in a 1D tensor that loses the batch structure, making it impossible to process multiple samples independently.</p>
</div>
<div class="paragraph">
<p>This behavior differs from frameworks like Keras, where flatten operations automatically preserve the batch dimension. Developers transitioning from Keras or those new to PyTorch often encounter this issue, leading to unexpected tensor shapes and model failures.</p>
</div>
<div class="paragraph">
<p>For example, if you have a tensor with shape <code>(32, 100, 100)</code> representing 32 images of size 100x100:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>torch.flatten(x)</code> produces shape <code>(320000,)</code> - all dimensions flattened</p>
</li>
<li>
<p><code>torch.flatten(x, start_dim=1)</code> produces shape <code>(32, 10000)</code> - batch dimension preserved</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The second approach maintains the batch structure, allowing proper batch processing in neural networks.</p>
</div>
<div class="sect2">
<h3 id="_what_is_the_potential_impact">What is the potential impact?</h3>
<div class="paragraph">
<p>Flattening the batch dimension can cause:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Model training failures due to incompatible tensor shapes</p>
</li>
<li>
<p>Silent bugs where the model processes data incorrectly</p>
</li>
<li>
<p>Performance issues when batch processing is broken</p>
</li>
<li>
<p>Difficult-to-debug shape mismatches in neural network layers</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_fix_in_pytorch">How to fix in PyTorch?</h3>
<div class="paragraph">
<p>Specify the <code>start_dim</code> parameter to preserve the batch dimension when flattening tensors.</p>
</div>
<div class="paragraph">
<p>If the first dimension should be also part of the flattening, it is a good practice to be explicit and specify <code>start_dim=0</code>.</p>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(32, 100, 100)  # Shape: (batch_size, height, width)
flattened = torch.flatten(x)  # Noncompliant: flattens all dimensions
# Result shape: (320000,) - batch dimension lost</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(32, 100, 100)  # Shape: (batch_size, height, width)
flattened = torch.flatten(x, start_dim=1)  # Preserves batch dimension
# Result shape: (32, 10000) - batch dimension preserved</code></pre>
</div>
</div>
<div class="paragraph">
<p>Use <code>nn.Flatten()</code> layer which preserves the batch dimension by default.</p>
</div>
</div>
<div class="sect3">
<h4 id="_non_compliant_code_example_2">Non-compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch

x = torch.randn(32, 100, 100)
flattened = x.flatten()  # Noncompliant: flattens all dimensions</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_compliant_code_example_2">Compliant code example</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import torch
import torch.nn as nn

x = torch.randn(32, 100, 100)
flatten_layer = nn.Flatten()  # Defaults to start_dim=1
flattened = flatten_layer(x)  # Preserves batch dimension</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_documentation">Documentation</h3>
<div class="ulist">
<ul>
<li>
<p>PyTorch torch.flatten documentation - <a href="https://pytorch.org/docs/stable/generated/torch.flatten.html">Official documentation for torch.flatten function</a></p>
</li>
<li>
<p>PyTorch nn.Flatten documentation - <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">Official documentation for nn.Flatten layer</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>