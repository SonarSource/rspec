== How to fix it in PyTorch

Specify the hyperparameters when instantiating the optimizer

=== Code examples

==== Noncompliant code example

[source,python,diff-id=2,diff-type=noncompliant]
----
from my_model import model
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr = 0.001) # Noncompliant : weight_decay is not specified, different values can change the behaviour of the optimizer significantly
----

==== Compliant solution

[source,python,diff-id=2,diff-type=compliant]
----
from my_model import model
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr = 0.001, weight_decay = 0.003) # Compliant
----
